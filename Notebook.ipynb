import pandas as pd
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier
from scipy.sparse import hstack
import gc
import warnings
import psutil
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, classification_report

warnings.filterwarnings('ignore')
print("Geliştirilmiş versiyon V8.0 (F1 Skoru Düzeltmeli) başlıyor...")

def check_memory():
    """RAM kullanımını kontrol eder."""
    ram_percent = psutil.virtual_memory().percent
    ram_used = psutil.virtual_memory().used // (1024**3)
    print(f"RAM: {ram_used}GB kullanılıyor ({ram_percent:.1f}%)")

# COĞRAFİ VERİLER 
ILLER = [
    'adana', 'adiyaman', 'afyonkarahisar', 'agri', 'amasya', 'ankara', 'antalya', 'artvin',
    'aydin', 'balikesir', 'bilecik', 'bingol', 'bitlis', 'bolu', 'burdur', 'bursa',
    'canakkale', 'cankiri', 'corum', 'denizli', 'diyarbakir', 'edirne', 'elazig', 'erzincan',
    'erzurum', 'eskisehir', 'gaziantep', 'giresun', 'gumushane', 'hakkari', 'hatay',
    'isparta', 'mersin', 'istanbul', 'izmir', 'kars', 'kastamonu', 'kayseri', 'kirklareli',
    'kirsehir', 'kocaeli', 'konya', 'kutahya', 'malatya', 'manisa', 'kahramanmaras',
    'mardin', 'mugla', 'mus', 'nevsehir', 'nigde', 'ordu', 'rize', 'sakarya', 'samsun',
    'siirt', 'sinop', 'sivas', 'tekirdag', 'tokat', 'trabzon', 'tunceli', 'sanliurfa',
    'usak', 'van', 'yozgat', 'zonguldak', 'aksaray', 'bayburt', 'karaman', 'kirikkale',
    'batman', 'sirnak', 'bartin', 'ardahan', 'igdir', 'yalova', 'karabuk', 'kilis',
    'osmaniye', 'duzce'
]

def process_address(address):
    """Basit ve etkili adres işleme - önceki başarılı versiyona dönüş"""
    if pd.isna(address): return ""
    text = str(address).lower()
    
    # karakter normalizasyon
    text = text.replace('i̇', 'i').replace('ı', 'i').replace('ğ', 'g').replace('ü', 'u')
    text = text.replace('ş', 's').replace('ö', 'o').replace('ç', 'c')
    
    replacements = {
        r'\bmah(\.|\s|$)': ' mahallesi ', r'\bmh(\.|\s|$)': ' mahallesi ',
        r'\bcad(\.|\s|$)': ' caddesi ', r'\bcd(\.|\s|$)': ' caddesi ',
        r'\bsok(\.|\s|$)': ' sokagi ', r'\bsk(\.|\s|$)': ' sokagi ',
        r'\bblv(\.|\s|$)': ' bulvari ', r'\bbulv(\.|\s|$)': ' bulvari ',
        r'\bapt(\.|\s|$)': ' apartmani ', r'\bapartman(\.|\s|$)': ' apartmani ', 
        r'\bno(\.|\s|$)': ' numara ', r'\bno\s*[:]\s*': ' numara ',
        r'\bkat(\.|\s|$)': ' kat ', r'\bk\s*[:]\s*': ' kat ', 
        r'\bdaire(\.|\s|$)': ' daire ', r'\bd\s*[:]\s*': ' daire ',
        r'\bsit(\.|\s|$)': ' sitesi ', r'\bsitesi(\.|\s|$)': ' sitesi ',
        r'\bbl(\.|\s|$)': ' blok ', r'\bblk(\.|\s|$)': ' blok ',
        'mahalesi': 'mahallesi', 'cadesi': 'caddesi'
    }
    
    for pattern, repl in replacements.items():
        text = re.sub(pattern, repl, text, flags=re.IGNORECASE)
    
    # tüm il adlarını işaretle
    for il in ILLER:
        if f" {il} " in f" {text} ":
            text = text.replace(il, f"il_{il}_il")
    
    # Noktalama işaretlerini temizle
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

try:
    train_path = "/kaggle/input/hepsiburada-hackathon-data/train.csv"
    test_path = "/kaggle/input/hepsiburada-hackathon-data/test.csv"
    train_df = pd.read_csv(train_path, names=["address","label"], header=0)
    test_df = pd.read_csv(test_path)
except FileNotFoundError:
    print("Uyarı: Kaggle dosya yolları bulunamadı. Yerel yollar kullanılıyor.")
    train_path = "train.csv"
    test_path = "test.csv"
    train_df = pd.read_csv(train_path, names=["address","label"], header=0)
    test_df = pd.read_csv(test_path)

print(f"Train verisi: {train_df.shape[0]} satır, Test verisi: {test_df.shape[0]} satır")
print(f"Benzersiz etiket sayısı: {train_df['label'].nunique()}")
print(f"Etiket dağılımı: \n{train_df['label'].value_counts().head(10)}")

# --- Veri İşleme ---
train_df['processed'] = train_df['address'].apply(process_address)
test_df['processed'] = test_df['address'].apply(process_address)
print("Veri işleme tamamlandı.")
check_memory()

# OFFLINE DEĞERLENDİRME 

print("Offline değerlendirme için train ve validation setleri oluşturuluyor...")
train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['label'])
print(f"Train: {train_data.shape[0]} satır, Validation: {val_data.shape[0]} satır")


print("TF-IDF Vektörleştiriciler eğitiliyor...")
# Tüm veriyi kullanarak vektörleştiricileri eğitme
all_text = pd.concat([train_df['processed'], test_df['processed']])

# BAŞARILI PARAMETRE SETİ - Deneysel olarak belirlenmiş
vectorizer_word = TfidfVectorizer(
    ngram_range=(1, 2),     # Tekli ve ikili kelime grupları
    max_features=8000,      # Özellik sayısını sınırla
    min_df=2,               # En az 2 dokümanda görülen terimler
    max_df=0.9,             # En fazla %90 dokümanda görülen terimler
    sublinear_tf=True,      # TF değerlerini logaritmik ölçekle yumuşat
    dtype=np.float32        # Bellek verimliliği
)

vectorizer_char = TfidfVectorizer(
    analyzer='char_wb',     # Kelime sınırları ile karakter n-gramları
    ngram_range=(3, 4),     # 3-4 karakter grupları
    max_features=10000,     # Özellik sayısını sınırla
    min_df=2,
    max_df=0.9,
    sublinear_tf=True,
    dtype=np.float32
)

print("Vektörleştiriciler eğitiliyor...")
vectorizer_word.fit(all_text)
vectorizer_char.fit(all_text)
del all_text
gc.collect()
print("Vektörleştiriciler eğitildi.")

print("Eğitim ve validation verileri vektörleştiriliyor...")
# Eğitim verisi için vektörleştirme
X_train_word = vectorizer_word.transform(train_data['processed'])
X_train_char = vectorizer_char.transform(train_data['processed'])
X_train_combined = hstack([X_train_word, X_train_char]).tocsr()

# Validation verisi için vektörleştirme
X_val_word = vectorizer_word.transform(val_data['processed'])
X_val_char = vectorizer_char.transform(val_data['processed'])
X_val_combined = hstack([X_val_word, X_val_char]).tocsr()

print("Vektörleştirme tamamlandı.")
check_memory()

# MODEL EĞİTİMİ 
print("Model eğitiliyor ve değerlendiriliyor...")
# SGDClassifier - Adres sınıflandırma için optimize edilmiş
model = SGDClassifier(
    loss='modified_huber',    # Olasılık tahminleri için daha iyi
    penalty='l2',            # Düzenlileştirme tipi
    alpha=0.0001,            # Düzenlileştirme gücü
    max_iter=100,            # Daha az iterasyon, aşırı öğrenmeyi önler
    tol=1e-3,                # Yakınsama toleransı
    class_weight='balanced', # Sınıf dengesizliğini ele al
    random_state=42,
    n_jobs=-1                # Tüm çekirdekleri kullan
)

model.fit(X_train_combined, train_data['label'])

# Validation seti
val_predictions = model.predict(X_val_combined)
val_f1 = f1_score(val_data['label'], val_predictions, average='weighted')
print(f"\n----- OFFLINE DEĞERLENDİRME SONUÇLARI -----")
print(f"Validation F1 skoru: {val_f1:.5f}")
print("\nDetaylı sınıflandırma raporu:")
print(classification_report(val_data['label'], val_predictions))
print("-----------------------------------------\n")

#Tüm train verisi

print("Final model tüm eğitim verisi ile eğitiliyor...")
# Tüm eğitim verisi için vektörleştirme
X_all_train_word = vectorizer_word.transform(train_df['processed'])
X_all_train_char = vectorizer_char.transform(train_df['processed'])
X_all_train_combined = hstack([X_all_train_word, X_all_train_char]).tocsr()

del X_train_word, X_train_char, X_val_word, X_val_char, X_train_combined, X_val_combined
gc.collect()

# Final model 
final_model = SGDClassifier(
    loss='modified_huber',
    penalty='l2',
    alpha=0.0001,
    max_iter=100,
    tol=1e-3,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)

final_model.fit(X_all_train_combined, train_df['label'])
print("Final model eğitimi tamamlandı!")
check_memory()

# TAHMİN 

print("Test verisi vektörleştiriliyor...")
X_test_word = vectorizer_word.transform(test_df['processed'])
X_test_char = vectorizer_char.transform(test_df['processed'])
X_test_combined = hstack([X_test_word, X_test_char]).tocsr()

print("Test verisi için tahminler yapılıyor...")
final_predictions = final_model.predict(X_test_combined)
print("Tahminler tamamlandı!")

# SUBMISSION 
print("\nSubmission dosyası oluşturuluyor...")
submission = test_df[['id']].copy()
submission['label'] = final_predictions
submission.to_csv('submission.csv', index=False)
check_memory()
print("Submission dosyası 'submission.csv' olarak kaydedildi!")

#Offline F1 skorunu
print(f"\nOffline Validation F1 Skoru: {val_f1:.5f}")
print("Bu skor, Kaggle üzerinde alacağınız F1 skorunun bir tahminidir.")
print("\nİşlem tamamlandı.")
